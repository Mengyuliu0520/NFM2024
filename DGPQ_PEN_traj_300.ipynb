{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983c7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import cos, sin\n",
    "from gym import spaces\n",
    "from gym.error import DependencyNotInstalled\n",
    "from typing import Optional\n",
    "from control.matlab import ss, lsim, linspace, c2d\n",
    "from functools import partial\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990103c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a5e4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_normalize(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d595d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pend1(gym.Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, g=10.0):\n",
    "        super(Pend1, self).__init__()\n",
    "        self.max_speed = 8\n",
    "        self.max_torque = 2.0\n",
    "        self.dt = 0.05\n",
    "        self.g = g\n",
    "        self.m = 1.0\n",
    "        self.l = 1.0\n",
    "        self.center = np.array([1,0,0])\n",
    "        self.obstacle = np.array([0,1,3])\n",
    "        self.render_mode = render_mode\n",
    "        self.screen_dim = 500\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "\n",
    "        high = np.array([1.0, 1.0, self.max_speed], dtype=np.float32)\n",
    "        # This will throw a warning in tests/envs/test_envs in utils/env_checker.py as the space is not symmetric\n",
    "        #   or normalised as max_torque == 2 by default. Ignoring the issue here as the default settings are too old\n",
    "        #   to update to follow the openai gym api\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-self.max_torque, high=self.max_torque, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "        # store current trace\n",
    "        self.cache1 = []\n",
    "        self.cache2 = []\n",
    "        self.cache3 = []\n",
    "        # ball radius\n",
    "        self.target_norm_radius = 0.2 # norm ball radius of target, tune this\n",
    "        self.safe_norm_radius = 0.1 # norm ball radius of safe, tune this\n",
    "        self.total_time = 120\n",
    "        # step number\n",
    "        self.steps = 0\n",
    "        self.u_lowbound = None\n",
    "        self.caches = []\n",
    "        self.reward_cache = [] # cache distances to target norm ball center\n",
    "#         self.avoid_reward_cache = [] # cache distances to obstacles norm ball center\n",
    "        self.final_reward_cache = [] # cache final reward\n",
    "        # How long should this trace be, i.e. deadline\n",
    "        self.step_const = random.randint(10, 50)\n",
    "        # Maximum reward from each trace\n",
    "        self.max_reward_list = []\n",
    "        self.quality_list = []\n",
    "        self.total_steps = 0\n",
    "        self.step_history = []\n",
    "        self.reached = False\n",
    "    def angle_normalize(x):\n",
    "        return ((x + np.pi) % (2 * np.pi)) - np.pi\n",
    "    def step(self, u):\n",
    "        th, thdot = self.state  # th := theta\n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        dt = self.dt\n",
    "        # simulate next step and get measurement\n",
    "        self.steps += 1\n",
    "        self.total_steps += 1\n",
    "        terminated = False\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u\n",
    "        costs = 1\n",
    "        self.last_u = u  # for rendering\n",
    "        costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
    "                                        \n",
    "        newthdot = thdot + (3 * g / (2 * l) * np.sin(th) + 3.0 / (m * l**2) * u) * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "        newth = th + newthdot * dt\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "\n",
    "        # calculate euclidean distance and update reward cache\n",
    "        dist = np.linalg.norm(self._get_obs() - self.center)\n",
    "        obs_dist = np.linalg.norm(self._get_obs() - self.obstacle)\n",
    "        reward = self.target_norm_radius - dist\n",
    "#         obs_reward = obs_dist-self.safe_norm_radius\n",
    "\n",
    "        self.reward_cache.append(reward)\n",
    "#         self.avoid_reward_cache.append(obs_reward)\n",
    "        # quantitative semantics\n",
    "        # reach reward, encourage reaching target\n",
    "        if self.steps < 10:\n",
    "            reach_reward = max(self.reward_cache)\n",
    "        else:\n",
    "            reach_reward = max(self.reward_cache[-10:])\n",
    "#         self.avoid_reward_cache.append(obs_reward)\n",
    "        # quantitative semantics\n",
    "        # reach reward, encourage reaching target\n",
    "#         if self.steps < 10:\n",
    "#             reach_reward = max(self.reward_cache)\n",
    "#         else:\n",
    "#             reach_reward = max(self.reward_cache[-10:])\n",
    "#         if self.steps < 10:\n",
    "#             avoid_reward = min(self.avoid_reward_cache)\n",
    "#         else:\n",
    "#             avoid_reward = min(self.avoid_reward_cache[-10:])\n",
    "\n",
    "#         # very strict reward, always within target\n",
    "#         strict_avoid_reward = avoid_reward - 0.5 * self.safe_norm_radius # half safe norm radius\n",
    "#         strict_reach_reward = reach_reward - 0.5 * self.target_norm_radius # half target norm radius\n",
    "\n",
    "        # overall reward, pick one of the final_reward\n",
    "#         final_reward = reach_reward\n",
    "#         final_reward = approach_reward\n",
    "#         final_reward = min(reach_reward, avoid_reward) # reach and avoid\n",
    "#         final_reward = min(approach_reward, avoid_reward) # approach and avoid\n",
    "#         final_reward = min(reach_reward, approach_reward) # reach and approach\n",
    "#         deadline_reward = (self.last_dist-dist)/(self.step_const - self.steps+1)\n",
    "        final_reward = reach_reward\n",
    "        # split cases: if already inside target, give very large constant reward for maintaining\n",
    "        if dist <= self.target_norm_radius:\n",
    "            final_reward = 10 # this gives 39/50 sucess with reach+approach+avoid\n",
    "\n",
    "        self.final_reward_cache.append(final_reward)\n",
    "\n",
    "        # If this is the last step, reset the state\n",
    "        if self.steps == self.step_const or obs_dist<=self.safe_norm_radius or dist>5:\n",
    "            self.max_reward_list.append(max(self.final_reward_cache)) # use max final reward to measure episodes\n",
    "            self.step_history.append(self.total_steps)\n",
    "            self.quality_list.append(sum(self.final_reward_cache))\n",
    "            terminated = True\n",
    "            self.reset()\n",
    "\n",
    "#         # If within target norm ball, early terminate\n",
    "#         if dist <= self.target_norm_radius:\n",
    "#             terminated = True\n",
    "#             self.reset()\n",
    "\n",
    "        # Return next state, reward, done, info\n",
    "        return self._get_obs(), final_reward, terminated, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.array([math.pi/4, -1])\n",
    "        self.reward_cache = []\n",
    "        self.final_reward_cache = []\n",
    "        self.steps=0\n",
    "        self.caches.append(self.cache1)\n",
    "        self.caches.append(self.cache2)\n",
    "        self.caches.append(self.cache3)\n",
    "        self.cache1 = []\n",
    "        self.cache2 = []\n",
    "        self.cache3 = []\n",
    "        # random # of steps for this trace\n",
    "#         self.step_const = random.randint(10, 50) # deadline range\n",
    "        self.step_const = 300\n",
    "        self.reached = False\n",
    "        return self._get_obs() # return something matching shape\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot], dtype=np.float32)\n",
    "\n",
    "    def render(self):\n",
    "        return\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed2d29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "#settings\n",
    "target_norm_radius = 0.2 # norm ball radius of target, tune this\n",
    "safe_norm_radius = 0.1\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "num_epi= 300\n",
    "num_steps = 200\n",
    "action_list = [-1,1]\n",
    "delta_tolerance = 2\n",
    "discount = 0.9\n",
    "center = np.array([1, 0, 0])\n",
    "obstacles = np.array([0, 1, 3])\n",
    "rmax = target_norm_radius\n",
    "mean_offset = target_norm_radius/(1-discount)\n",
    "# initial the env\n",
    "env = Pend1()\n",
    "state = env.reset()\n",
    "lpschitz = 2\n",
    "#Initial data collection\n",
    "initial_buffer = 50\n",
    "epsilon = 0.005\n",
    "data_buffer = [[],[]]\n",
    "for i in range(initial_buffer):\n",
    "    this_action = random.randint(0,1)\n",
    "    new_state, reward, done, _= env.step([action_list[this_action]])\n",
    "    data_buffer[this_action].append((state, reward))\n",
    "    state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee3e628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx_1 = np.array([tuple[0] for tuple in data_buffer[0]])\n",
    "trainx_2 = np.array([tuple[0] for tuple in data_buffer[1]])\n",
    "trainy_1 = np.array([tuple[1] for tuple in data_buffer[0]])\n",
    "trainy_2 = np.array([tuple[1] for tuple in data_buffer[1]])\n",
    "train_x_list = [trainx_1, trainx_2]\n",
    "train_y_list = [trainy_1, trainy_2]\n",
    "x_scaler_list = []\n",
    "for i in range(len(action_list)):\n",
    "    x_scaler = StandardScaler()\n",
    "    train_x_list[i] = x_scaler.fit_transform(train_x_list[i])\n",
    "    x_scaler_list.append(x_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c48ae3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    }
   ],
   "source": [
    "gpc1 = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "gpc2 = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "reward_q1 = []\n",
    "reward_q2 = []\n",
    "gpclist = [gpc1,gpc2]\n",
    "reward_q_list = [reward_q1,reward_q2]\n",
    "i = 0\n",
    "for i in range(len(action_list)):\n",
    "    gpclist[i].fit(train_x_list[i], train_y_list[i])\n",
    "results = [0]*2\n",
    "new_results = [0]*2\n",
    "std_list_1 = [0]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb4a5df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     results[k], std_list_1[k] \u001b[38;5;241m=\u001b[39m gpclist[k]\u001b[38;5;241m.\u001b[39mpredict(state_entry_scale, return_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     lp_values \u001b[38;5;241m=\u001b[39m [point[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mlpschitz\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(point[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m state_entry) \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m reward_q_list[k]]\n\u001b[0;32m     13\u001b[0m     results[k], std_list_1[k] \u001b[38;5;241m=\u001b[39m gpclist[k]\u001b[38;5;241m.\u001b[39mpredict(state_entry_scale, return_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m     results[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmin\u001b[39m(lp_values), results[k])\n",
      "Cell \u001b[1;32mIn[33], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m     results[k], std_list_1[k] \u001b[38;5;241m=\u001b[39m gpclist[k]\u001b[38;5;241m.\u001b[39mpredict(state_entry_scale, return_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     lp_values \u001b[38;5;241m=\u001b[39m [point[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mlpschitz\u001b[38;5;241m*\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_entry\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m reward_q_list[k]]\n\u001b[0;32m     13\u001b[0m     results[k], std_list_1[k] \u001b[38;5;241m=\u001b[39m gpclist[k]\u001b[38;5;241m.\u001b[39mpredict(state_entry_scale, return_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m     results[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmin\u001b[39m(lp_values), results[k])\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\linalg.py:2526\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2524\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x_real\u001b[38;5;241m.\u001b[39mdot(x_real) \u001b[38;5;241m+\u001b[39m x_imag\u001b[38;5;241m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2526\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2527\u001b[0m ret \u001b[38;5;241m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for i in range(num_epi):\n",
    "    print(i)\n",
    "    for j in range(num_steps):\n",
    "        # max q_st\n",
    "        for k in range(len(reward_q_list)):\n",
    "            state_entry = state.reshape(1,-1)\n",
    "            state_entry_scale = x_scaler_list[k].transform(state_entry)\n",
    "            if reward_q_list[k] == []:\n",
    "                results[k], std_list_1[k] = gpclist[k].predict(state_entry_scale, return_std=True)\n",
    "            else:\n",
    "                lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - state_entry) for point in reward_q_list[k]]\n",
    "                results[k], std_list_1[k] = gpclist[k].predict(state_entry_scale, return_std=True)\n",
    "                results[k] = min(min(lp_values), results[k])\n",
    "                \n",
    "#         if reward_q1 == []:\n",
    "#             state_entry = state.reshape(1,-1)\n",
    "#             value_result1, std1 = gpc1.predict(state_entry, return_std=True)\n",
    "#         else:\n",
    "#             state_entry = state.reshape(1,-1)\n",
    "#             lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - state_entry) for point in reward_q1]\n",
    "#             value_result1, std1 = gpc1.predict(state_entry, return_std=True)\n",
    "#             value_result1 = min(min(lp_values), value_result1)\n",
    "#         if reward_q2 == []:\n",
    "#             state_entry = state.reshape(1,-1)\n",
    "#             value_result2, std2 = gpc2.predict(state_entry, return_std=True)\n",
    "#         else:\n",
    "#             state_entry = state.reshape(1,-1)\n",
    "#             lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - state_entry) for point in reward_q2]\n",
    "#             value_result2, std2 = gpc2.predict(state_entry, return_std=True)\n",
    "#             value_result2 = min(min(lp_values), value_result2)        \n",
    "#         if reward_q3 == []:\n",
    "#             state_entry = state.reshape(1,-1)\n",
    "#             value_result3, std3 = gpc3.predict(state_entry, return_std=True)\n",
    "#         else:\n",
    "#             state_entry = state.reshape(1,-1)\n",
    "#             lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - state_entry) for point in reward_q3]\n",
    "#             value_result3, std3 = gpc3.predict(state_entry, return_std=True)\n",
    "#             value_result3 = min(min(lp_values), value_result3)        \n",
    "#         if reward_q4 == []:\n",
    "#             state_entry = state.reshape(1,-1)\n",
    "#             value_result4, std4 = gpc4.predict(state_entry, return_std=True)\n",
    "#         else:\n",
    "#             state_entry = state.reshape(1,-1)\n",
    "#             lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - state_entry) for point in reward_q4]\n",
    "#             value_result4, std4 = gpc4.predict(state_entry, return_std=True)\n",
    "#             value_result4 = min(min(lp_values), value_result4)\n",
    "        # get actions\n",
    "        # get new values\n",
    "#         results = np.array([value_result1, value_result2, value_result3, value_result4])\n",
    "#         results = np.array([x+mean_offset for x in results])\n",
    "        which_action = action_list[np.argmax(results)]\n",
    "        new_state, reward, done, _ = env.step([which_action])\n",
    "        k = 0\n",
    "        for k in range(len(reward_q_list)):\n",
    "            new_state_entry = new_state.reshape(1,-1)\n",
    "            new_state_entry_scale = x_scaler_list[k].transform(new_state_entry)\n",
    "            if reward_q_list[k] == []:\n",
    "                new_results[k], std_list_1[k] = gpclist[k].predict(new_state_entry_scale, return_std=True)\n",
    "            else:\n",
    "                lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - new_state_entry) for point in reward_q_list[k]]\n",
    "                new_results[k], std_list_1[k] = gpclist[k].predict(new_state_entry_scale, return_std=True)\n",
    "                new_results[k] = min(min(lp_values), new_results[k])\n",
    "                \n",
    "        \n",
    "#         if reward_q1 == []:\n",
    "#             new_state_entry = new_state.reshape(1,-1)\n",
    "#             new_value_result1 = gpc1.predict(new_state_entry)[0]\n",
    "#         else:\n",
    "#             new_state_entry = new_state.reshape(1,-1)\n",
    "#             lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - new_state_entry) for point in reward_q1]\n",
    "#             new_value_result1 = gpc1.predict(new_state_entry)[0]\n",
    "#             new_value_result1 = min(min(lp_values), new_value_result1)\n",
    "#         if reward_q2 == []:\n",
    "#             new_state_entry = new_state.reshape(1,-1)\n",
    "#             new_value_result2 = gpc2.predict(new_state_entry)[0]\n",
    "#         else:\n",
    "#             new_state_entry = new_state.reshape(1,-1)\n",
    "#             lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - new_state_entry) for point in reward_q2]\n",
    "#             new_value_result2 = gpc2.predict(new_state_entry)[0]\n",
    "#             new_value_result2 = min(min(lp_values), new_value_result2)        \n",
    "#         if reward_q3 == []:\n",
    "#             new_state_entry = new_state.reshape(1,-1)\n",
    "#             new_value_result3 = gpc3.predict(new_state_entry)[0]\n",
    "#         else:\n",
    "#             new_state_entry = new_state.reshape(1,-1)\n",
    "#             lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - new_state_entry) for point in reward_q3]\n",
    "#             new_value_result3 = gpc3.predict(new_state_entry)[0]\n",
    "#             new_value_result3 = min(min(lp_values), new_value_result3)        \n",
    "#         if reward_q4 == []:\n",
    "#             new_state_entry = state.reshape(1,-1)\n",
    "#             new_value_result4 = gpc4.predict(new_state_entry)[0]\n",
    "#         else:\n",
    "#             new_state_entry = state.reshape(1,-1)\n",
    "#             lp_values = [point[1]+lpschitz*np.linalg.norm(point[0] - new_state_entry) for point in reward_q4]\n",
    "#             new_value_result4 = gpc4.predict(new_state_entry)[0]\n",
    "#             new_value_result4 = min(min(lp_values), new_value_result4)\n",
    "#         # get the q value\n",
    "#         new_results = np.array([new_value_result1, new_value_result2, new_value_result3, new_value_result4])\n",
    "#         new_results = np.array([x+mean_offset for x in new_results])\n",
    "        \n",
    "        q_value = reward+discount*max(new_results)\n",
    "        action_index = np.argmax(results)\n",
    "        var1 = std_list_1[action_index]**2\n",
    "#         if which_action == -10:\n",
    "#             var1 = std1*std1\n",
    "#         elif which_action == -5:\n",
    "#             var1 = std2*std2\n",
    "#         elif which_action == 5:\n",
    "#             var1 = std3*std3\n",
    "#         else:\n",
    "#             var1 = std4*std4\n",
    "        if var1>delta_tolerance:\n",
    "            if isinstance(q_value, np.ndarray):\n",
    "                q_value = q_value[0]\n",
    "            train_x_list[action_index] = np.vstack([train_x_list[action_index], state_entry])\n",
    "            train_y_list[action_index] = np.concatenate([train_y_list[action_index], [q_value]])\n",
    "            x_scaler = StandardScaler()\n",
    "            train_x_list[action_index] = x_scaler.fit_transform(train_x_list[action_index])\n",
    "            x_scaler_list[action_index] = x_scaler\n",
    "            gpclist[action_index].fit(train_x_list[action_index], train_y_list[action_index])\n",
    "        state_entry_new_scale = x_scaler_list[action_index].transform(state_entry)\n",
    "        useless_result,update_std = gpclist[action_index].predict(state_entry_new_scale, return_std=True)\n",
    "        var2 = update_std*update_std\n",
    "        if (abs(results[action_index]-useless_result))>2*epsilon:\n",
    "            reward_q_list[action_index].append((state, useless_result+epsilon))\n",
    "            for k in range(len(action_list)):\n",
    "                gpclist[k] = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "                train_x_list[k] = train_x_list[k][:2]\n",
    "                train_y_list[k] = train_y_list[k][:2]\n",
    "        state= new_state\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3324943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number reached = 0\n",
      "Total number collison = 0\n",
      "Total number Stay = 0\n",
      "Average reach time = 50.0\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# Test 50 traces\n",
    "env = Pend1()\n",
    "state = env.reset()\n",
    "dims0 = []\n",
    "dims1 = []\n",
    "dims2 = []\n",
    "euclids = []\n",
    "center = np.array([1, 0, 0])\n",
    "obstacle = np.array([0, 1, 3])\n",
    "num_reached = 0\n",
    "unsafe = 0\n",
    "time = 0\n",
    "stay = 0\n",
    "for i in range(len(action_list)):\n",
    "    reward_q_list[i].append((np.array([0,0,0]), np.array([-5])))\n",
    "for j in range(20):\n",
    "    dim0 = []\n",
    "    dim1 = []\n",
    "    dim2 = []\n",
    "    euclid = []\n",
    "    state = env.reset()\n",
    "    flag = 0\n",
    "    # Print initial state\n",
    "    for i in range(50):\n",
    "        q1_euclids = np.array([np.linalg.norm(state[:3]-x[0])for x in reward_q1])\n",
    "        q2_euclids = np.array([np.linalg.norm(state[:3]-x[0])for x in reward_q2])\n",
    "        close_1 = np.argmin(q1_euclids)\n",
    "        close_2 = np.argmin(q2_euclids)\n",
    "        close_value_list = np.array([reward_q1[close_1][1],reward_q2[close_2][1]])\n",
    "        action = [action_list[np.argmin(close_value_list)]]\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        dim0.append(state[0])\n",
    "        dim1.append(state[1])\n",
    "        dim2.append(state[2])\n",
    "        dist = np.linalg.norm(state[:3]-center)\n",
    "        obs_dist = np.linalg.norm(state[:3]-obstacle)\n",
    "        euclid.append(dist)\n",
    "        state = new_state\n",
    "        if obs_dist <= 0.1:\n",
    "            unsafe+=1\n",
    "            time+=50\n",
    "            break\n",
    "        if dist <= 0.2: # stop\n",
    "            if flag == 0:\n",
    "                num_reached += 1\n",
    "                time+=i\n",
    "                flag = 1\n",
    "            else:\n",
    "                if i == 49:\n",
    "                    stay+=1\n",
    "                continue\n",
    "    dims0.append(dim0)\n",
    "    dims1.append(dim1)\n",
    "    dims2.append(dim2)\n",
    "    euclids.append(euclid)\n",
    "ref= [math.pi/2]*100\n",
    "time+=(1000-num_reached-unsafe)*50\n",
    "print(\"Total number reached = \" + str(num_reached))\n",
    "print(\"Total number collison = \" + str(unsafe))\n",
    "print(\"Total number Stay = \" + str(stay))\n",
    "print(\"Average reach time = \" + str(time/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a89d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "np.save('q_tab_pen_asap_300', reward_q_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf437f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
